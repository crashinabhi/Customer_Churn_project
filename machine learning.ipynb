{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import our input dataset\n",
    "customer_churn_df = pd.read_csv('Final_Data.csv')\n",
    "customer_churn_small_df=customer_churn_df.sample(100000)\n",
    "customer_churn_small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn_df = customer_churn_df.query('10 < age < 80')\n",
    "customer_churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn_df.dropna()\n",
    "customer_churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn_small_df=customer_churn_df.sample(100000)\n",
    "customer_churn_small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_churn_small_df.dropna()\n",
    "customer_churn_small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn_small_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn_small_df.to_csv(r'C:\\Users\\avhad\\Desktop\\AnalyticsData\\BootCamp\\Module20\\Customer_Churn_project\\Customer_Churn_project\\Customer_Churn_project\\customer_churn_sample.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Csv\n",
    "file_path= \"Customer_Churn_database.csv\"\n",
    "customer_churn_df=pd.read_csv(file_path)\n",
    "customer_churn_small=customer_churn_df.sample(100000)\n",
    "customer_churn_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to pgAdmin and fet Customer_churn table \n",
    "connection=psycopg2.connect(host=\"localhost\", \n",
    "                            dbname=\"Customer_Churn\",\n",
    "                            user=\"postgres\",\n",
    "                           password=\"Carolina#92\")\n",
    "cur=connection.cursor()\n",
    "cur.execute(\"select * from Customer_churn \")\n",
    "all=cur.fetchall()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform database into dataframe \n",
    "column_names=[\"city\", \"gender\", \"registered_via\", \"registered_on\",\"date_streamed\",\"less_than_25\",\"less_than_50\",\"less_than_75\",\"less_than_985\",\"above_985\",\"unique_songs\",\"total_secs\",\"payment_method_id\",\"payment_plan_days\",\"plan_list_price\",\"actual_amount_paid\",\"auto_renewed\",\"transaction_date\",\"membership_expire_date\",\"cancelled\",\"is_churn\"]\n",
    "customer_churn_small_df.head()\n",
    "customer_churn_small_df=pd.DataFrame(all, columns=column_names)\n",
    "customer_churn_small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_dummies() to convert categorical variables.\n",
    "customer_churn_dummies= pd.get_dummies(customer_churn_small_df,columns=[\"gender\",\"cancelled\",\"auto_renewed\",\"payment_method_id\"])\n",
    "customer_churn_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variables for modeling\n",
    "y=customer_churn_dummies['is_churn'].values\n",
    "X=customer_churn_dummies.drop(columns = ['is_churn','customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data with MinMaxScaler()\n",
    "features = X.columns.values\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "X.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Train & dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "                                max_iter=300,\n",
    "                                random_state=1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the weights of all the variables\n",
    "weights = pd.Series(model.coef_[0],\n",
    "                 index=X.columns.values)\n",
    "print (weights.sort_values(ascending = False).plot(kind='bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive realtions of weight ( Increasing the probability of Churn)\n",
    "weights = pd.Series(model.coef_[0],\n",
    "                 index=X.columns.values)\n",
    "print (weights.sort_values(ascending = False)[:18].plot(kind='bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative realtions of weight ( Decreasing the probability of Churn)\n",
    "weights = pd.Series(model.coef_[0],\n",
    "                 index=X.columns.values)\n",
    "print (weights.sort_values(ascending = False)[25:].plot(kind='bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, prediction_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to TabPy server using the client library\n",
    "connection = tabpy_client.Client('http://localhost:9004/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow curve to define the best number of kplots \n",
    "from sklearn.cluster import KMeans\n",
    "inertia=[]\n",
    "k=list(range(1,8))\n",
    "\n",
    "# Looking for the best K\n",
    "for i in k:\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(customer_churn_dummies)\n",
    "inertia.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame to plot the Elbow Curve using hvPlot\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n",
    "df_elbow.hvplot.line(x=\"k\", y=\"inertia\", title=\"Elbow Curve\", xticks=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(k, data):\n",
    "   # Create a copy of the DataFrame\n",
    "   data = data.copy()\n",
    "\n",
    "   # Initialize the K-Means model\n",
    "   model = KMeans(n_clusters=k, random_state=0)\n",
    "\n",
    "   # Fit the model\n",
    "   model.fit(data)\n",
    "\n",
    "   # Predict clusters\n",
    "   predictions = model.predict(data)\n",
    "\n",
    "   # Create return DataFrame with predicted clusters\n",
    "   data[\"class\"] = model.labels_\n",
    "\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most important variable predictors of churn.\n",
    "weights_forest = model_rf.feature_importances_\n",
    "weights = pd.Series(weights_forest,\n",
    "                 index=X.columns.values)\n",
    "weights.sort_values()[-10:].plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vecor Machine (SVM)\n",
    "#Create train & dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SVM model\n",
    "from sklearn.svm import SVC\n",
    "model.svm = SVC(kernel='linear') \n",
    "model.svm.fit(X_train,y_train)\n",
    "preds = model.svm.predict(X_test)\n",
    "metrics.accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,preds))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
